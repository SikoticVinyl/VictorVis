{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Refinement\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we focus on optimizing and refining the models developed in previous steps. While the initial models provided a good baseline, further fine-tuning is required to improve their performance and ensure they generalize well to new data.\n",
    "\n",
    "### Objectives:\n",
    "- **Hyperparameter Tuning**: We will explore different sets of hyperparameters using methods such as grid search or random search to identify the best configurations for each model.\n",
    "- **Cross-Validation**: To ensure the models are robust and not overfitting, we will use cross-validation to evaluate their performance on multiple data subsets.\n",
    "- **Model Comparison**: After optimization, we will compare the models based on key metrics such as accuracy, precision, recall, F1-score, and more, ensuring we select the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-split data\n",
    "X_train = joblib.load('../outputs/X_train_encoded.joblib')\n",
    "X_test = joblib.load('../outputs/X_test_encoded.joblib')\n",
    "y_train = joblib.load('../outputs/y_train.joblib')\n",
    "y_test = joblib.load('../outputs/y_test.joblib')\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    return mse, r2, mae\n",
    "\n",
    "# Initialize results tracking\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and evaluating baseline models...\n",
      "Baseline RandomForest - Test MSE: 0.0018, Test R2: 0.9003, Test MAE: 0.0314, CV MSE: 0.0016\n",
      "Baseline XGBoost - Test MSE: 0.0021, Test R2: 0.8804, Test MAE: 0.0344, CV MSE: 0.0018\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate baseline models\n",
    "print(\"Loading and evaluating baseline models...\")\n",
    "baseline_models = {\n",
    "    'RandomForest': joblib.load('../models/random_forest_model.joblib'),\n",
    "    'XGBoost': joblib.load('../models/xgboost_model.joblib')\n",
    "}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    train_mse, train_r2, train_mae = evaluate_model(model, X_train, y_train)\n",
    "    test_mse, test_r2, test_mae = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_mse = -cv_scores.mean()\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Iteration': 'Baseline',\n",
    "        'Train MSE': train_mse,\n",
    "        'Train R2': train_r2,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MSE': test_mse,\n",
    "        'Test R2': test_r2,\n",
    "        'Test MAE': test_mae,\n",
    "        'CV MSE': cv_mse\n",
    "    })\n",
    "    print(f\"Baseline {name} - Test MSE: {test_mse:.4f}, Test R2: {test_r2:.4f}, Test MAE: {test_mae:.4f}, CV MSE: {cv_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization stages\n",
    "optimization_stages = [\n",
    "    {\n",
    "        'name': 'Basic Tuning',\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Advanced Tuning',\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [300, 400, 500],\n",
    "            'max_depth': [20, 30, 40, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [300, 400, 500],\n",
    "            'max_depth': [4, 5, 6, 7],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Basic Tuning...\n",
      "Optimizing RandomForest...\n",
      "Optimizing XGBoost...\n",
      "\n",
      "Performing Advanced Tuning...\n",
      "Optimizing RandomForest...\n",
      "Optimizing XGBoost...\n"
     ]
    }
   ],
   "source": [
    "# Perform iterative optimization\n",
    "optimized_models = baseline_models.copy()\n",
    "for stage in optimization_stages:\n",
    "    print(f\"\\nPerforming {stage['name']}...\")\n",
    "    for name, model in optimized_models.items():\n",
    "        print(f\"Optimizing {name}...\")\n",
    "        random_search = RandomizedSearchCV(\n",
    "            model, stage[name], n_iter=20, cv=5, n_jobs=-1, \n",
    "            random_state=42, scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        random_search.fit(X_train, y_train)\n",
    "        optimized_models[name] = random_search.best_estimator_\n",
    "        \n",
    "        train_mse, train_r2, train_mae = evaluate_model(optimized_models[name], X_train, y_train)\n",
    "        test_mse, test_r2, test_mae = evaluate_model(optimized_models[name], X_test, y_test)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(optimized_models[name], X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        cv_mse = -cv_scores.mean()\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Iteration': stage['name'],\n",
    "            'Train MSE': train_mse,\n",
    "            'Train R2': train_r2,\n",
    "            'Train MAE': train_mae,\n",
    "            'Test MSE': test_mse,\n",
    "            'Test R2': test_r2,\n",
    "            'Test MAE': test_mae,\n",
    "            'CV MSE': cv_mse\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Optimization Results:\n",
      "+---+--------------+-----------------+------------------------+--------------------+-----------------------+-----------------------+--------------------+----------------------+-----------------------+\n",
      "|   |    Model     |    Iteration    |       Train MSE        |      Train R2      |       Train MAE       |       Test MSE        |      Test R2       |       Test MAE       |        CV MSE         |\n",
      "+---+--------------+-----------------+------------------------+--------------------+-----------------------+-----------------------+--------------------+----------------------+-----------------------+\n",
      "| 0 | RandomForest |    Baseline     | 0.00021605887022184737 | 0.9882479728654439 | 0.010577342047930252  | 0.0017650239598568358 | 0.9003449235890976 | 0.03137962401455429  | 0.0015982510600238017 |\n",
      "| 1 |   XGBoost    |    Baseline     | 1.949556903478577e-06  | 0.9998939583197556 | 0.0009426877035487339 | 0.0021176921084175805 | 0.8804329155417036 | 0.034353495119333416 | 0.0017778491323245612 |\n",
      "| 2 | RandomForest |  Basic Tuning   | 0.00020293120189534018 | 0.9889620223012674 | 0.010410185607404152  | 0.0018326346087701737 | 0.8965275564955676 | 0.03205343305707164  | 0.0015728687341150824 |\n",
      "| 3 |   XGBoost    |  Basic Tuning   | 0.00018152100987604502 | 0.990126580633487  | 0.009628295926655091  | 0.001884286890017996  | 0.8936112153287504 | 0.03155942085610023  | 0.0015995213158885445 |\n",
      "| 4 | RandomForest | Advanced Tuning | 0.00020032649140114655 | 0.9891036995597571 | 0.010368191721133057  | 0.0018379180648204357 | 0.8962292471080593 | 0.032098847786537384 | 0.0015662793866623836 |\n",
      "| 5 |   XGBoost    | Advanced Tuning | 0.0001891532763959688  | 0.9897114409859076 | 0.009938988509928273  | 0.0017270127230363818 | 0.9024910772935053 | 0.030170736577996574 | 0.0015649208699424853 |\n",
      "+---+--------------+-----------------+------------------------+--------------------+-----------------------+-----------------------+--------------------+----------------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "# Create and display results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Optimization Results:\")\n",
    "print(tabulate(results_df, headers='keys', tablefmt='pretty', floatfmt='.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "\n",
      "RandomForest:\n",
      "  Baseline  - MSE: 0.0018, R2: 0.9003, MAE: 0.0314, CV MSE: 0.0016\n",
      "  Optimized - MSE: 0.0018, R2: 0.8962, MAE: 0.0321, CV MSE: 0.0016\n",
      "  Improvement - MSE: -4.13%, R2: -0.46%, MAE: -2.29%, CV MSE: 2.00%\n",
      "\n",
      "XGBoost:\n",
      "  Baseline  - MSE: 0.0021, R2: 0.8804, MAE: 0.0344, CV MSE: 0.0018\n",
      "  Optimized - MSE: 0.0017, R2: 0.9025, MAE: 0.0302, CV MSE: 0.0016\n",
      "  Improvement - MSE: 18.45%, R2: 2.51%, MAE: 12.18%, CV MSE: 11.98%\n"
     ]
    }
   ],
   "source": [
    "# Display final model performance and comparison\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "for name in baseline_models.keys():\n",
    "    baseline_results = results_df[(results_df['Model'] == name) & (results_df['Iteration'] == 'Baseline')].iloc[0]\n",
    "    optimized_results = results_df[(results_df['Model'] == name) & (results_df['Iteration'] == 'Advanced Tuning')].iloc[0]\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Baseline  - MSE: {baseline_results['Test MSE']:.4f}, R2: {baseline_results['Test R2']:.4f}, MAE: {baseline_results['Test MAE']:.4f}, CV MSE: {baseline_results['CV MSE']:.4f}\")\n",
    "    print(f\"  Optimized - MSE: {optimized_results['Test MSE']:.4f}, R2: {optimized_results['Test R2']:.4f}, MAE: {optimized_results['Test MAE']:.4f}, CV MSE: {optimized_results['CV MSE']:.4f}\")\n",
    "    \n",
    "    mse_improvement = (baseline_results['Test MSE'] - optimized_results['Test MSE']) / baseline_results['Test MSE'] * 100\n",
    "    r2_improvement = (optimized_results['Test R2'] - baseline_results['Test R2']) / baseline_results['Test R2'] * 100\n",
    "    mae_improvement = (baseline_results['Test MAE'] - optimized_results['Test MAE']) / baseline_results['Test MAE'] * 100\n",
    "    cv_mse_improvement = (baseline_results['CV MSE'] - optimized_results['CV MSE']) / baseline_results['CV MSE'] * 100\n",
    "    \n",
    "    print(f\"  Improvement - MSE: {mse_improvement:.2f}%, R2: {r2_improvement:.2f}%, MAE: {mae_improvement:.2f}%, CV MSE: {cv_mse_improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to 'first_model_optimization_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('../outputs/first_model_optimization_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'first_model_optimization_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: XGBoost (Iteration: Advanced Tuning)\n",
      "Best Model Performance:\n",
      "  MSE: 0.0017\n",
      "  R2: 0.9025\n",
      "  MAE: 0.0302\n",
      "  CV MSE: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model = results_df.loc[results_df['Test R2'].idxmax()]\n",
    "print(f\"\\nBest Model: {best_model['Model']} (Iteration: {best_model['Iteration']})\")\n",
    "print(f\"Best Model Performance:\")\n",
    "print(f\"  MSE: {best_model['Test MSE']:.4f}\")\n",
    "print(f\"  R2: {best_model['Test R2']:.4f}\")\n",
    "print(f\"  MAE: {best_model['Test MAE']:.4f}\")\n",
    "print(f\"  CV MSE: {best_model['CV MSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final optimized models\n",
    "for name, model in optimized_models.items():\n",
    "    joblib.dump(model, f'../models/optimized_{name.lower()}_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
